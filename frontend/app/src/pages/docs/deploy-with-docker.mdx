# Deploy with Docker Compose

This document provides instructions for deploying the entire RAG using Docker Compose.

## Deploy

> **Prerequisites:**
>
> 1. Set up a [TiDB Serverless cluster](https://docs.pingcap.com/tidbcloud/tidb-cloud-quickstart).
> 2. Install [Docker Compose](https://docs.docker.com/compose/install/).

1. Clone the repository:

    ```bash
    git clone https://github.com/pingcap/autoflow.git;
    cd autoflow/;
    ```

2. Select an embedding model for pingcap/autoflow.

    We recommend using the OpenAI `text-embedding-3-small` model for Autoflow due to its performance and compatibility. However, other supported embedding models can also be utilized.

    - OpenAI
        - **Model:** `text-embedding-3-small`
            - **Embedding Dimensions:** 1536
            - **Maximum Tokens (EMBEDDING_MAX_TOKENS):** 8191
    - JinaAI
        - **Model:** `jina-clip-v1`
            - **Embedding Dimensions:** 768
            - **Maximum Tokens (EMBEDDING_MAX_TOKENS):** 8192
        - Additional models can be found at [JinaAI Embeddings](https://jina.ai/embeddings/).
    - Local Embedding Server
        - **Model:** `BAAI/bge-m3`
            - **Embedding Dimensions:** 1024
            - **Maximum Tokens (EMBEDDING_MAX_TOKENS):** 8192

    We also support embedding models that conform to the OpenAI API format. You can select the "OpenAI-Like" option to use your own embedding model. Details for specific models are provided below:

    - ZhipuAI (Default Option for OpenAI-Like)
        - **Model:** `embedding-3`
            - **Embedding Dimensions:** 2048
            - **Maximum Tokens (EMBEDDING_MAX_TOKENS):** 8192
        - More information is available at [ZhipuAI API Documentation](https://open.bigmodel.cn/dev/api/vector/embedding-3).
        - **Configuration Note:** Since ZhipuAI is the default option for OpenAI-Like models, no additional configuration is required in the "Advanced Settings".
    - Other OpenAI-Like Models
        - **Set EMBEDDING_MAX_TOKENS**: Refer to the model's API documentation to find the correct values for and `EMBEDDING_MAX_TOKENS`.
        - **Configure API Base in "Advanced Settings"**:

            Add the following configuration in JSON format:

            ```json
            {
                "api_base": "{api_base}",
            }
            ```

            <Callout>
            OpenAI and ZhipuAI models have predefined configurations in their types. These examples are provided for reference when integrating other models.
            </Callout>

            - Example 1: For OpenAI's embedding API (`https://api.openai.com/v1/embeddings`), the `api_base` should be set to `https://api.openai.com/v1`, resulting in the following configuration:

                ```json
                {
                    "api_base": "https://api.openai.com/v1"
                }
                ```

            - Example 2: For ZhipuAI's embedding API (`https://open.bigmodel.cn/api/paas/v4/embeddings`), the `api_base` should be set to `https://open.bigmodel.cn/api/paas/v4`, resulting in the following configuration:

                ```json
                {
                    "api_base": "https://open.bigmodel.cn/api/paas/v4"
                }
                ```

import { Callout } from 'nextra/components'

<Callout>
Note: You cannot change the embedding model after deployment because different models have varying vector dimensions and generate different vectors for the same input text.
If you want to use a different embedding model after deployment, you need to recreate the app and database.
</Callout>

1. Copy and edit the `.env` file:

    ```bash
    cp .env.example .env
    vim .env # or use another text editor to edit this file
    ```

    Replace the following placeholders with your own values:
    - `SECRET_KEY`: you can generate a random secret key using `python3 -c "import secrets; print(secrets.token_urlsafe(32))"`
    - `TIDB_HOST`, `TIDB_USER`, `TIDB_PASSWORD` and `TIDB_DATABASE`: get them from your [TiDB Serverless cluster](https://tidbcloud.com/)
      - Note: TiDB Serverless will provide a default database name called `test`, if you want to use another database name, you need to create a new database in the TiDB Serverless console.
      - Note: Don't use '#' in the password, it will cause an error when connecting to the database. See [issue](https://github.com/pydantic/pydantic/issues/8061).
    - `EMBEDDING_MAX_TOKENS`: set them according to the embedding model you choose before, it can not be changed after the deployment.

2. Migrate the database schema:

    ```bash
    docker compose run backend /bin/sh -c "alembic upgrade head"
    ```

3. Bootstrap the database with initial data:

    ```bash
    docker compose run backend /bin/sh -c "python bootstrap.py"
    ```

    Running the bootstrap script creates an admin user. You can find the username and password in the output.

4. Start the services:

    ```bash
    docker compose up
    ```

    To use the local embedding model, start with the following command:

    ```bash
    docker compose --profile local-embedding-reranker up
    ```

5. Open your browser and visit `http://localhost:3000` to access the web interface.

That's it! You can now use pingcap/autoflow locally. You can also go to https://tidb.ai to experience the live demo.


## Configuration

After you deploy the application, you need to initialize the application through the following steps:

* Set up the default LLM model.
* Set up the default Embedding model.
* Set up `Data Source` to index the data.

![initialization](https://github.com/user-attachments/assets/7f9253da-3d6f-4ccd-838d-feed3f0b6f05 "Initialization")


## Upgrade

This section will help you upgrade pingcap/autoflow to the new version.

Suppose you want to upgrade pingcap/autoflow from 0.1.0 to version 0.2.0

1. Edit your docker-compose.yml file to use the new image version.

    ```yaml
    services:
      backend:
        image: tidbai/backend:0.2.0
      frontend:
        image: tidbai/frontend:0.2.0
      background:
        image: tidbai/backend:0.2.0
    ```

2. Pull the new image:

    ```bash
    docker compose pull
    ```

3. Migrate the database schema:

    ```bash
    docker compose run backend /bin/sh -c "alembic upgrade head"
    ```

4. Recreate the docker containers:

    ```bash
    docker compose up -d --force-recreate
    ```

5. Check the logs to ensure everything is working correctly:

    ```bash
    docker compose logs -f
    ```

6. Done!
